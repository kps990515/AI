![img.png](img.png)
![img_1.png](img_1.png)
![img_2.png](img_2.png)
![img_3.png](img_3.png)
![img_4.png](img_4.png)
![img_5.png](img_5.png)
![img_6.png](img_6.png)
![img_7.png](img_7.png)
![img_8.png](img_8.png)

## RNNì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì˜í™” ë¦¬ë·°ì˜ ê°ì • ë¶„ì„ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í•™ìŠµ
![img_10.png](img_10.png)
![img_11.png](img_11.png)
![img_12.png](img_12.png)
![img_13.png](img_13.png)
![img_14.png](img_14.png)
```python
######################################################################
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ & ë°ì´í„°ì…‹ ë¡œë“œ
######################################################################
!pip install datasets                     # ğŸ¤— Datasets: ê³µê°œ ë§ë­‰ì¹˜ ì‰½ê²Œ ë¡œë“œ
# Colabì²˜ëŸ¼ GPU í™˜ê²½ì´ë©´ ì´ë¯¸ torch / matplotlib ì„¤ì¹˜ë¼ ìˆì„ ê²ƒ

import torch
from datasets import load_dataset

# IMDB ì˜í™” ë¦¬ë·° ë°ì´í„° (í›ˆë ¨ 25,000ê°œ, í…ŒìŠ¤íŠ¸ 25,000ê°œ)
ds = load_dataset("stanfordnlp/imdb")
print(ds)                                 # train / test ë¡œ ë¶„í• ëœ DatasetDict
print(ds['train'][0])                     # ğŸ‘‰ {'text': '...', 'label': 0}
```

```python
######################################################################
# 1. WordPiece í† í¬ë‚˜ì´ì € ì§ì ‘ í•™ìŠµ
######################################################################
from tokenizers import (
    models,            # WordPiece / BPE ë“± í† í°í™” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    normalizers,       # ì •ê·œí™” ê·œì¹™ (ì†Œë¬¸ìí™”, UNICODE ì²˜ë¦¬ ë“±)
    pre_tokenizers,    # "ë„ì–´ì“°ê¸°Â·êµ¬ë‘ì " ë‹¨ìœ„ë¡œ 1ì°¨ ë¶„í• 
    trainers,          # ì‹¤ì œ vocab í•™ìŠµì„ ë‹´ë‹¹í•˜ëŠ” ê°ì²´
    Tokenizer,         # í† í¬ë‚˜ì´ì € ì»¨í…Œì´ë„ˆ
)

# 1â€‘1) tokenizer ê³¨ê²© ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))  # '[UNK]' : OOV í† í°
tokenizer.normalizer    = normalizers.BertNormalizer(lowercase=True)  # ëŒ€ë¬¸ìâ†’ì†Œë¬¸ì, HTMLÂ Strip ë“±
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()           # ë„ì–´ì“°ê¸°/êµ¬ë‘ì  ë¶„ë¦¬

# 1â€‘2) í•™ìŠµìš© í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´í„°
#      ë°ì´í„°ì…‹ì´ 25k ë¬¸ì¥ â†’ ë©”ëª¨ë¦¬ì— í•œêº¼ë²ˆì— ì˜¬ë¦¬ì§€ ì•Šê¸° ìœ„í•´ 1,000ê°œì”© ì˜ë¼ generatorë¡œ ê³µê¸‰
def get_training_corpus():
    for i in range(0, len(ds['train']), 1000):
        # slicing ì€ ì‚¬ë³¸ì„ ë§Œë“¤ì§€ ì•Šê³  meta ì •ë³´ë§Œ ìœ ì§€í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        yield ds['train'][i : i + 1000]['text']   # => List[str]

# 1â€‘3) WordPiece Trainer ì„¤ì •
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]"]      # BERT ê³„ì—´ í•„ìˆ˜ í† í°
trainer = trainers.WordPieceTrainer(
    vocab_size=10_000,             # ìµœëŒ€ ì‚¬ì „ í¬ê¸° (íŠ¹ìˆ˜í† í° í¬í•¨)
    special_tokens=special_tokens  # ìœ„ ë„¤ ê°œ í† í°ì€ ë¬´ì¡°ê±´ vocabì— í¬í•¨
)

# 1â€‘4) ì‹¤ì œ í•™ìŠµ ìˆ˜í–‰ (iterator ë¥¼ í˜ë ¤ë³´ëƒ„)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)

# í•™ìŠµ ê²°ê³¼ í™•ì¸
print(tokenizer.encode("Hello, world!").tokens)
#   ì˜ˆ) ['hello', ',', 'world', '!']
```

```python
######################################################################
# 2. HuggingFace Transformers ë˜í¼(BertTokenizerFast)ë¡œ ê°ì‹¸ê¸°
#    â†’ .__call__() ë§Œìœ¼ë¡œ paddingÂ·truncation ë“±ì„ í•œ ë²ˆì— ì²˜ë¦¬
######################################################################
from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)  # ë‚´ë¶€ì— ë°©ê¸ˆ í•™ìŠµí•œ tokenizer íƒ‘ì¬
print("vocab size :", len(tokenizer))                      # 10,000 + íŠ¹ìˆ˜í† í° ìˆ˜
print("PAD id     :", tokenizer.pad_token_id)              # == 0
```

```python
######################################################################
# 3. ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„ í™•ì¸ (íŒ¨ë”© ê¸¸ì´ ê²°ì •ì— ë„ì›€)
######################################################################
from matplotlib import pyplot as plt

# ì „ì²´ train ì…‹ì„ í† í°í™”í•˜ì—¬ ê¸¸ì´ ìˆ˜ì§‘ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ â†’ ì¼ë¶€ ìƒ˜í”Œë§ OK)
lengths = [len(tokenizer(ex['text']).input_ids) for ex in ds['train']]
plt.hist(lengths, bins=50); plt.title("Token length (train)"); plt.show()
print("í‰ê·  ê¸¸ì´:", sum(lengths)/len(lengths))             # ëŒ€ëµ 230~250

# â†’ ëŒ€ë¶€ë¶„ 400 í† í° ì´í•˜ì´ë¯€ë¡œ max_len = 400 ìœ¼ë¡œ ì„¤ì •
```
![img_9.png](img_9.png)

```python
######################################################################
# 4. DataLoader ì¤€ë¹„ (ë¬¸ì¥ â†’ íŒ¨ë”©ëœ LongTensor)
# ì…ë ¥ë“¤ì˜ listê°€ ì£¼ì–´ì¡Œì„ ë•Œ textë“¤ì€ tokenizeì™€ paddingì„ ë™ì‹œì— ì§„í–‰í•˜ì—¬ í•˜ë‚˜ì˜ matrixë¡œ ë§Œë“œëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„
######################################################################
from torch.utils.data import DataLoader

def collate_fn(batch, max_len=400):
    """
    batch : List[dict]  (ê° dictëŠ” {"text": str, "label": int})
    return: ids -> LongTensor (B, max_len)
            labels -> LongTensor (B,)
    """
    # truncationê³¼ max_lengthëŠ” ê¸¸ì´ê°€ max_lengthë¥¼ ë„˜ëŠ” dataë“¤ì€ max_lengthì—ì„œ ìë¥´ë¼ëŠ” ì˜µì…˜
    texts  = [ex['text']  for ex in batch]
    labels = [ex['label'] for ex in batch]

    # tokenizer(...) ëŠ” dict ë°˜í™˜ (input_ids, attention_mask ë“±)
    ids = tokenizer(
        texts,
        padding=True,               # ê°€ì¥ ê¸´ ë¬¸ì¥ ê¸¸ì´ì— ë§ì¶° PAD ì‚½ì…
        truncation=True,            # max_len ì´ˆê³¼ ì‹œ ë’·ë¶€ë¶„ ìë¥´ê¸°
        max_length=max_len
    ).input_ids                     # List[List[int]]

    return torch.LongTensor(ids), torch.LongTensor(labels)

train_loader = DataLoader(ds['train'], batch_size=64, shuffle=True,
                          collate_fn=collate_fn)
test_loader  = DataLoader(ds['test'],  batch_size=64, shuffle=False,
                          collate_fn=collate_fn)

# ì²« ë°°ì¹˜ shape í™•ì¸
x_batch, y_batch = next(iter(train_loader))
print(x_batch.shape)   # (64, 400)  â† (ë°°ì¹˜, ì‹œí€€ìŠ¤ ê¸¸ì´)
print(y_batch.shape)   # (64,)
```

```python
######################################################################
# 5. í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ (Embedding â†’ RNN â†’ Linear)
# 1) Token listë¥¼ nn.Embeddingìœ¼ë¡œ ì „ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.
# 2) nn.RNNìœ¼ë¡œ ì„ ì–¸ëœ RNNì„ í†µê³¼ì‹œì¼œ representationì„ ì–»ìŠµë‹ˆë‹¤.
# 3) ì£¼ì–´ì§„ token listì˜ ë§ˆì§€ë§‰ tokenì— í•´ë‹¹í•˜ëŠ” representationì„ ì–»ì–´ nn.Linearë¥¼ í†µê³¼ì‹œì¼œ ì¶œë ¥ì„ êµ¬í•©ë‹ˆë‹¤.
######################################################################
from torch import nn

class TextClassifier(nn.Module):
    """
    - Embedding : ë‹¨ì–´ ID â†’ ë²¡í„°
    - RNN       : ìˆœë°©í–¥ ë‹¨ìˆœ RNN(ReLU)  (â€» ì‹¤ì œë¡  LSTM/GRUê°€ ì„±ëŠ¥â†‘)
    - classifier: ë§ˆì§€ë§‰ timeâ€‘step hidden â†’ ë¡œì§“(ê¸/ë¶€ì •)
    """
    def __init__(self, vocab_size, hidden_dim=32, n_layers=2):
        super().__init__()

        # padding_idx=0 : [PAD] ë²¡í„°ëŠ” í•™ìŠµì—ì„œ ì œì™¸ (gradient X)
        self.embedding  = nn.Embedding(vocab_size, hidden_dim,
                                       padding_idx=tokenizer.pad_token_id)
        # batch_first=True : ì…ë ¥ shape (B, L, H)
        self.rnn        = nn.RNN(hidden_dim, hidden_dim,
                                 num_layers=n_layers,
                                 nonlinearity='relu',
                                 batch_first=True)
        self.classifier = nn.Linear(hidden_dim, 1)   # 1 ë¡œì§“ â†’ BCE Loss

    def forward(self, ids):
        """
        ids : LongTensor (B, L)
        """
        x = self.embedding(ids)      # (B, L, H)
        x, _ = self.rnn(x)           # RNN ê²°ê³¼ ì „ì²´ time step h_t

        # ğŸ”¸ ë¬¸ì¥ë§ˆë‹¤ 'ì‹¤ì œ ë§ˆì§€ë§‰ í† í°' ìœ„ì¹˜(index) êµ¬í•´ ê·¸ hiddenë§Œ ì‚¬ìš©
        lengths = (ids != tokenizer.pad_token_id).sum(dim=-1) - 1  # (B,) 0â€‘based
        lengths = lengths.unsqueeze(-1).unsqueeze(-1)              # shape (B,1,1)
        lengths = lengths.expand(-1, 1, x.size(-1))                # (B,1,H)
        last_h  = x.gather(dim=1, index=lengths)[:, 0]             # (B, H)

        return self.classifier(last_h)     # (B, 1)   (raw logit)

model = TextClassifier(len(tokenizer)).cuda()
print(model(x_batch.cuda()).shape)         # (64, 1)
```

```python
######################################################################
# 6. í•™ìŠµ ì¤€ë¹„ (ì†ì‹¤í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €)
######################################################################
from torch.optim import Adam

loss_fn  = nn.BCEWithLogitsLoss()          # ë¡œì§“ + ì‹œê·¸ëª¨ì´ë“œ í•©ì¹œ BCE
optimizer = Adam(model.parameters(), lr=1e-3)

######################################################################
# 7. ì •í™•ë„ ê³„ì‚° í—¬í¼
######################################################################
def accuracy(model, loader):
    correct = total = 0
    for ids, labels in loader:
        ids, labels = ids.cuda(), labels.cuda()
        with torch.no_grad():
            logits = model(ids)
            preds  = (logits > 0).long().squeeze(-1)  # 0/1 ë¡œ ë³€í™˜
        total   += labels.size(0)
        correct += (preds == labels).sum().item()
    return correct / total

######################################################################
# 8. í•™ìŠµ ë£¨í”„
######################################################################
n_epochs = 5        # ë°ëª¨ ìš©ë„ë¡œ 5 epoch; ì‹¤ì œ 50 epochê¹Œì§€ ëŒë ¤ë„ OK

for epoch in range(1, n_epochs + 1):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    model.train()
    running_loss = 0.0
    for ids, labels in train_loader:
        ids     = ids.cuda()
        labels  = labels.cuda().float()    # BCE â‡’ float í•„ìš”

        optimizer.zero_grad()              # grad ì´ˆê¸°í™”
        logits = model(ids).squeeze(-1)    # (B,)
        loss   = loss_fn(logits, labels)   # BCE
        loss.backward()                    # ì—­ì „íŒŒ
        optimizer.step()                   # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

        running_loss += loss.item()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ eval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    model.eval()
    tr_acc = accuracy(model, train_loader)
    te_acc = accuracy(model, test_loader)

    print(f"[{epoch}/{n_epochs}] "
          f"loss {running_loss:.2f} | train_acc {tr_acc:.3f} | test_acc {te_acc:.3f}")
```
![img_15.png](img_15.png)
![img_16.png](img_16.png)
![img_17.png](img_17.png)
![img_18.png](img_18.png)
![img_19.png](img_19.png)
![img_20.png](img_20.png)
![img_21.png](img_21.png)
![img_22.png](img_22.png)
![img_23.png](img_23.png)
![img_24.png](img_24.png)
![img_25.png](img_25.png)
![img_26.png](img_26.png)
![img_27.png](img_27.png)
