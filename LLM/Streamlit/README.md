# Streamlit RAG Chatbot --- ì½”ë“œ ì£¼ì„ ê¸°ë°˜ í•´ì„¤ (chat.py / llm.py / config.py)

ìš”ì²­ì‚¬í•­: **ì„¤ëª…ì€ ë¬¸ì„œ ì„œìˆ ì´ ì•„ë‹ˆë¼ "ì½”ë“œ ì£¼ì„"ìœ¼ë¡œ ì½”ë“œ ì•ˆì— ì§ì ‘
í‘œê¸°**í–ˆìŠµë‹ˆë‹¤.\
ë”°ë¼ì„œ ì•„ë˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ë©´, ì½”ë“œ ìì²´ê°€ í•™ìŠµ ìë£Œê°€ ë©ë‹ˆë‹¤.

------------------------------------------------------------------------

## 0) ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

-   `.env`ì— ìµœì†Œ ì•„ë˜ í‚¤ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    -   `OPENAI_API_KEY=...`
    -   `PINECONE_API_KEY=...`
-   Pineconeì— `tax-markdown-index` ì¸ë±ìŠ¤ê°€ **ì´ë¯¸ ì¡´ì¬**í•˜ê³ , í•´ë‹¹
    ì¸ë±ìŠ¤ì— ë¬¸ì„œê°€ **ì—…ì„œíŠ¸**ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
-   Streamlit ì‹¤í–‰:
    -   `streamlit run chat.py`

------------------------------------------------------------------------

# chat.py (annotated)

``` python
import streamlit as st  # Streamlit UI í”„ë ˆì„ì›Œí¬
from dotenv import load_dotenv  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ ìœ í‹¸
from llm import get_ai_response  # ë°±ì—”ë“œ(RAG+LLM) ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

# Streamlit ì•±ì˜ ê¸°ë³¸ ë©”íƒ€ ì •ë³´(ë¸Œë¼ìš°ì € íƒ­ ì œëª©/ì•„ì´ì½˜)
st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")

# í™”ë©´ ìƒë‹¨ íƒ€ì´í‹€/ì„¤ëª… ë¬¸êµ¬
st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

# .env ë¡œë“œ: OPENAI_API_KEY, PINECONE_API_KEY ë“± ì‹¤í–‰ì— í•„ìš”í•œ í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…
load_dotenv()

# Streamlitì€ ê¸°ë³¸ì ìœ¼ë¡œ 'ë§¤ ì¸í„°ë™ì…˜ë§ˆë‹¤ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰'í•˜ë¯€ë¡œ,
# ëŒ€í™” ì´ë ¥ì€ st.session_stateì— ì €ì¥í•´ì•¼ ìœ ì§€ë©ë‹ˆë‹¤.
if "message_list" not in st.session_state:
    st.session_state.message_list = []  # [{"role": "user"|"ai", "content": "..."}]

# ì €ì¥ëœ ê³¼ê±° ë©”ì‹œì§€ë“¤ì„ í™”ë©´ì— ë‹¤ì‹œ ë Œë”ë§ (ìƒˆë¡œê³ ì¹¨/ì¬ì‹¤í–‰ì—ë„ UI ìœ ì§€)
for message in st.session_state.message_list:
    # Streamlitì˜ chat UI ì»¨í…Œì´ë„ˆ (roleì— ë”°ë¼ ë§í’ì„  ìŠ¤íƒ€ì¼ì´ ë°”ë€œ)
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ìê°€ ì…ë ¥ì°½ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  Enterë¥¼ ì¹˜ë©´ user_questionì— ê°’ì´ ë“¤ì–´ì˜µë‹ˆë‹¤.
# (ì•„ë¬´ ì…ë ¥ë„ ì—†ìœ¼ë©´ None/Falseë¡œ í‰ê°€ë˜ì–´ ì•„ë˜ ë¸”ë¡ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    # 1) ì‚¬ìš©ì ì…ë ¥ì„ UIì— ì¦‰ì‹œ í‘œì‹œ
    with st.chat_message("user"):
        st.write(user_question)

    # 2) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì— ì €ì¥ (ë‹¤ìŒ rerun ë•Œë„ ìœ ì§€)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    # 3) ë°±ì—”ë“œ í˜¸ì¶œ ë™ì•ˆ ë¡œë”© ìŠ¤í”¼ë„ˆ í‘œì‹œ
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        # get_ai_responseëŠ” "ìŠ¤íŠ¸ë¦¬ë° iterator/generator"ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ì¦‰, í•œ ë²ˆì— ë¬¸ìì—´ì„ ë°˜í™˜í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ í† í°/ì²­í¬ ë‹¨ìœ„ë¡œ yield í•©ë‹ˆë‹¤.
        ai_response_stream = get_ai_response(user_question)

        # 4) AI ë§í’ì„  ì˜ì—­ì— ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        with st.chat_message("ai"):
            # st.write_streamì€ generatorë¥¼ ë°›ì•„ì„œ ë“¤ì–´ì˜¤ëŠ” ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— ì¶œë ¥í•˜ê³ ,
            # ìµœì¢…ì ìœ¼ë¡œ í™”ë©´ì— ì¶œë ¥ëœ ì „ì²´ ë¬¸ìì—´(ëˆ„ì  ê²°ê³¼)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            full_answer_text = st.write_stream(ai_response_stream)

        # 5) ìµœì¢… ëˆ„ì  ë‹µë³€ì„ ì„¸ì…˜ì— ì €ì¥ (ë‹¤ìŒ rerunì— ê·¸ëŒ€ë¡œ ì¬í‘œì‹œ)
        st.session_state.message_list.append({"role": "ai", "content": full_answer_text})
```

------------------------------------------------------------------------

# llm.py (annotated)

``` python
# ---- ì¶œë ¥ íŒŒì„œ / í”„ë¡¬í”„íŠ¸ ìœ í‹¸ ----
from langchain_core.output_parsers import StrOutputParser  # LLM ì¶œë ¥(AIMessage ë“±) -> ë¬¸ìì—´ë¡œ ë³€í™˜
from langchain_core.prompts import (
    ChatPromptTemplate,                 # chat promptë¥¼ êµ¬ì„±í•˜ëŠ” í…œí”Œë¦¿
    MessagesPlaceholder,                # chat_history ê°™ì€ "ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸"ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…
    FewShotChatMessagePromptTemplate,   # few-shot ì˜ˆì‹œ(ì§ˆë¬¸/ë‹µë³€)ë“¤ì„ chat ë©”ì‹œì§€ í˜•íƒœë¡œ ì‚½ì…
)

# ---- ì²´ì¸ êµ¬ì„± ìš”ì†Œ ----
from langchain.chains import (
    create_history_aware_retriever,  # ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•´ "ë…ë¦½ ì§ˆë¬¸"ìœ¼ë¡œ ì¬ì‘ì„± í›„ retrieval
    create_retrieval_chain,          # retriever + ë¬¸ì„œê²°í•©(qa) ì²´ì¸ì„ í•©ì³ RAG ì²´ì¸ì„ êµ¬ì„±
)
from langchain.chains.combine_documents import create_stuff_documents_chain
# create_stuff_documents_chain:
#   - retrieved docsë¥¼ promptì˜ {context} ìë¦¬ì— "ê·¸ëŒ€ë¡œ(stuff)" ë„£ê³ 
#   - LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” combine_docs_chain ìƒì„±

# ---- ëª¨ë¸/ë²¡í„°ìŠ¤í† ì–´ ----
from langchain_openai import ChatOpenAI          # OpenAI Chat LLM ë˜í¼
from langchain_openai import OpenAIEmbeddings   # OpenAI ì„ë² ë”© ëª¨ë¸ ë˜í¼
from langchain_pinecone import PineconeVectorStore  # Pinecone ì¸ë±ìŠ¤ë¥¼ LangChain VectorStoreë¡œ ì‚¬ìš©

# ---- ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬(ì„¸ì…˜) ----
from langchain_community.chat_message_histories import ChatMessageHistory  # in-memory ë©”ì‹œì§€ ì €ì¥ êµ¬í˜„ì²´
from langchain_core.chat_history import BaseChatMessageHistory            # íˆìŠ¤í† ë¦¬ ì¸í„°í˜ì´ìŠ¤(ì¶”ìƒ)
from langchain_core.runnables.history import RunnableWithMessageHistory   # ì²´ì¸ ì‹¤í–‰ì— ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ìë™ ì£¼ì…

# ---- few-shot ì˜ˆì‹œ ë¡œë“œ ----
from config import answer_examples  # config.pyì— ì •ì˜ëœ few-shot (input/answer) ë¦¬ìŠ¤íŠ¸

# Streamlit ì„¸ì…˜ê³¼ ë³„ê°œë¡œ, LangChain ìª½ì—ì„œë„ "ì„¸ì…˜ë³„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬"ë¥¼ ê´€ë¦¬í•´ì•¼
# create_history_aware_retriever, MessagesPlaceholder("chat_history") ë“±ì´ ì œëŒ€ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ íŒŒì´ì¬ dictì— ì„¸ì…˜ë³„ ChatMessageHistoryë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
store = {}  # {session_id: ChatMessageHistory()}


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """ì„¸ì…˜ IDë³„ ChatMessageHistoryë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
    - RunnableWithMessageHistoryê°€ ì´ í•¨ìˆ˜ë¥¼ í†µí•´ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒ/ê°±ì‹ í•©ë‹ˆë‹¤.
    """
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


def get_retriever():
    """Pinecone VectorStore ê¸°ë°˜ Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - Embedding ëª¨ë¸: text-embedding-3-large
    - Pinecone index: tax-markdown-index (ë¯¸ë¦¬ êµ¬ì¶•ë˜ì–´ ìˆì–´ì•¼ í•¨)
    - k=4: top-4 ë¬¸ì„œ ì¡°ê°ì„ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    """
    embedding = OpenAIEmbeddings(model="text-embedding-3-large")

    # Pineconeì— ë¯¸ë¦¬ ìƒì„±ë˜ì–´ ìˆëŠ” ì¸ë±ìŠ¤ëª…
    index_name = "tax-markdown-index"

    # from_existing_index: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ì— "ì ‘ì†"í•˜ëŠ” í˜•íƒœ (ë°ì´í„° upsertëŠ” ë³„ë„)
    database = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding
    )

    # retriever: .invoke(query)ë¡œ ë¬¸ì„œ ì¡°ê°(Document[])ì„ ë°˜í™˜
    retriever = database.as_retriever(search_kwargs={"k": 4})
    return retriever


def get_llm(model: str = "gpt-4o"):
    return ChatOpenAI(model=model)


def get_history_retriever():
    """History-aware Retriever ìƒì„±.
    ëª©ì :
    - ëŒ€í™”í˜• ì±—ë´‡ì—ì„œ ì‚¬ìš©ìê°€ 'ê·¸ê±°', 'ì•„ê¹Œ ë§í•œ', 'ìœ„ ë‚´ìš©' ê°™ì´ ì§€ì‹œì–´ë¥¼ ì“°ë©´
      ê·¸ëŒ€ë¡œ ë²¡í„°ê²€ìƒ‰í•˜ë©´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ê·¸ë˜ì„œ ë¨¼ì € LLMìœ¼ë¡œ 'ë…ë¦½ ì§ˆë¬¸(standalone question)'ìœ¼ë¡œ ì¬ì‘ì„±í•œ í›„
      ê·¸ ì§ˆë¬¸ìœ¼ë¡œ retrieverë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

    êµ¬ì„±:
    - contextualize_q_prompt: (system + chat_history + latest input) -> standalone question ìƒì„± í”„ë¡¬í”„íŠ¸
    - create_history_aware_retriever: (llm, retriever, prompt)ë¥¼ ë¬¶ì–´ history-aware retriever ë°˜í™˜
    """
    llm = get_llm()
    retriever = get_retriever()

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            # ì•„ë˜ placeholderì— RunnableWithMessageHistoryê°€ session_idì— í•´ë‹¹í•˜ëŠ” íˆìŠ¤í† ë¦¬ë¥¼ ìë™ ì£¼ì…
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),  # ìµœì‹  ì‚¬ìš©ì ì…ë ¥
        ]
    )

    # ë°˜í™˜ë˜ëŠ” history_aware_retrieverëŠ” ë‚´ë¶€ì ìœ¼ë¡œ:
    # 1) llmìœ¼ë¡œ standalone question ìƒì„±
    # 2) retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰
    # ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever


def get_dictionary_chain():
    """ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „(Dictionary) ê¸°ë°˜ ì§ˆë¬¸ ë³´ì • ì²´ì¸.
    ëª©ì :
    - ì‚¬ìš©ì í‘œí˜„(ì‚¬ëŒ/ì§ì¥ì¸ ë“±)ì„ KB ìš©ì–´(ê±°ì£¼ì ë“±)ë¡œ ì •ê·œí™”í•´ retrieval recall/precisionì„ ê°œì„ .
    - ì´ ì˜ˆì œì—ì„œëŠ” ì•„ì£¼ ì‘ì€ ì‚¬ì „ 1ê°œë§Œ ì‚¬ìš©í•˜ì§€ë§Œ, ì‹¤ë¬´ì—ì„œëŠ” ë‹¤ìˆ˜ ê·œì¹™/ë™ì˜ì–´/ì•½ì–´ í…Œì´ë¸”ë¡œ í™•ì¥í•©ë‹ˆë‹¤.

    ë™ì‘:
    - prompt | llm | StrOutputParser()
    - ì…ë ¥ í‚¤: question
    - ì¶œë ¥: ë³´ì •ëœ ì§ˆë¬¸ ë¬¸ìì—´
    """
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]
    llm = get_llm()

    # f-stringì„ ì‚¬ìš©í•´ ì‚¬ì „ì„ í”„ë¡¬í”„íŠ¸ì— ë°•ì•„ ë„£ì—ˆìŠµë‹ˆë‹¤.
    # (ì‚¬ì „ì´ ì»¤ì§€ë©´: ì™¸ë¶€ íŒŒì¼/DBë¡œ ë¶„ë¦¬ + í† í° ì ˆì•½ì„ ìœ„í•œ êµ¬ì¡°í™” ê¶Œì¥)
    prompt = ChatPromptTemplate.from_template(f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”

ì‚¬ì „: {dictionary}

ì§ˆë¬¸: {{question}}
""")

    dictionary_chain = prompt | llm | StrOutputParser()
    return dictionary_chain


def get_rag_chain():
    """(History-aware Retriever + Few-shot + System Prompt) ê¸°ë°˜ RAG ì²´ì¸ ìƒì„±.

    í° íë¦„:
    1) Few-shot(ì˜ˆì‹œ Q/A) + system ì§€ì¹¨ + chat_history + user input ìœ¼ë¡œ QA í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    2) create_stuff_documents_chain: retrieved docsë¥¼ {context}ë¡œ 'stuff'í•˜ì—¬ LLMì— ì „ë‹¬
    3) create_retrieval_chain: history-aware retriever + combine_docs_chain ê²°í•©
    4) RunnableWithMessageHistory: session_id ê¸°ë°˜ìœ¼ë¡œ chat_historyë¥¼ ìë™ ìœ ì§€/ì£¼ì…
    5) pick('answer'): ìµœì¢… ê²°ê³¼ dictì—ì„œ answerë§Œ ìŠ¤íŠ¸ë¦¼/ë°˜í™˜
    """
    llm = get_llm()

    # ---- Few-shot ì„¤ì • ----
    # example_promptëŠ” "í•œ ê°œ ì˜ˆì‹œ"ì˜ í…œí”Œë¦¿ì…ë‹ˆë‹¤.
    # examples=answer_examplesëŠ” config.pyì—ì„œ ê°€ì ¸ì˜¨ ë¦¬ìŠ¤íŠ¸ì´ë©°,
    # few_shot_promptê°€ ì´ë¥¼ ë°˜ë³µ ì‚½ì…í•©ë‹ˆë‹¤.
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )

    # ---- System prompt (í–‰ë™ ê·œì¹™/í†¤/í˜•ì‹) ----
    system_prompt = (
        "ë‹¹ì‹ ì€ ì†Œë“ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì†Œë“ì„¸ë²•ì— ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œë¥¼ í™œìš©í•´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "ë‹µë³€ì„ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ë‹µë³€ì„ ì œê³µí•  ë•ŒëŠ” ì†Œë“ì„¸ë²• (XXì¡°)ì— ë”°ë¥´ë©´ ì´ë¼ê³  ì‹œì‘í•˜ë©´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "2-3 ë¬¸ì¥ì •ë„ì˜ ì§§ì€ ë‚´ìš©ì˜ ë‹µë³€ì„ ì›í•©ë‹ˆë‹¤"
        "\n\n"
        "{context}"  # create_stuff_documents_chainê°€ ì—¬ê¸°ì— retrieved docsë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
    )

    # ---- ìµœì¢… QA Prompt ----
    # MessagesPlaceholder("chat_history")ê°€ ë“¤ì–´ê°€ ìˆìœ¼ë¯€ë¡œ multi-turn ëŒ€í™”ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # ---- Retrieval + QA ê²°í•© ----
    history_aware_retriever = get_history_retriever()

    # combine_docs_chain: (docs + prompt) -> llm -> answer
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # rag_chain: (input) -> (retrieverë¡œ docs) -> (docs+inputìœ¼ë¡œ answer)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # ---- ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì—°ê²° ----
    # RunnableWithMessageHistoryê°€ session_idë³„ë¡œ chat_historyë¥¼ ìë™ ê´€ë¦¬í•©ë‹ˆë‹¤.
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",          # ì‚¬ìš©ìì˜ ì…ë ¥ í‚¤
        history_messages_key="chat_history", # í”„ë¡¬í”„íŠ¸ placeholder í‚¤
        output_messages_key="answer",        # rag_chain ê²°ê³¼ dictì—ì„œ ë‹µë³€ í‚¤
    ).pick("answer")  # ìµœì¢…ì ìœ¼ë¡œ answer ë¬¸ìì—´ë§Œ ë°˜í™˜/ìŠ¤íŠ¸ë¦¼

    return conversational_rag_chain


def get_ai_response(user_message: str):
    """Streamlitì—ì„œ í˜¸ì¶œí•˜ëŠ” 'ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ' ì—”íŠ¸ë¦¬í¬ì¸íŠ¸.

    ì—¬ê¸°ì„œ í•˜ëŠ” ì¼:
    1) dictionary_chainìœ¼ë¡œ user_message(ì§ˆë¬¸)ë¥¼ KB ìš©ì–´ë¡œ ë³´ì •
    2) ë³´ì •ëœ ì§ˆë¬¸ì„ inputìœ¼ë¡œ rag_chain ì‹¤í–‰
    3) .stream()ìœ¼ë¡œ í† í°/ì²­í¬ ë‹¨ìœ„ generatorë¥¼ ë°˜í™˜ (Streamlitì´ ì´ë¥¼ ì‹¤ì‹œê°„ ì¶œë ¥)

    âš ï¸ ì£¼ì˜(í‚¤ ë§¤í•‘):
    - dictionary_chainì€ ì…ë ¥ í‚¤ë¡œ {question}ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    - rag_chainì€ ì…ë ¥ í‚¤ë¡œ {input}ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    - ê·¸ë˜ì„œ LCELì—ì„œ {"input": dictionary_chain}ë¡œ "dictionary ê²°ê³¼ë¥¼ inputì— ë°”ì¸ë”©"í•©ë‹ˆë‹¤.
    """
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()

    # LCEL ë§¤í•‘:
    # - dictionary_chainì˜ ì¶œë ¥(ë³´ì •ëœ ì§ˆë¬¸ ë¬¸ìì—´)ì„ rag_chainì˜ "input"ìœ¼ë¡œ ì—°ê²°
    # - dictionary_chainì€ ì‹¤í–‰ ì‹œ {"question": ...}ë¥¼ ë°›ì•„ì•¼ í•¨
    tax_chain = {"input": dictionary_chain} | rag_chain

    # stream(): generatorë¥¼ ë°˜í™˜. Streamlitì˜ st.write_streamì´ ì´ë¥¼ ë°›ì•„ ì‹¤ì‹œê°„ í‘œì‹œ.
    ai_response_stream = tax_chain.stream(
        {"question": user_message},
        config={
            # RunnableWithMessageHistoryê°€ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ëŠ” í‚¤
            "configurable": {"session_id": "abc123"}
        },
    )

    return ai_response_stream
```

------------------------------------------------------------------------

# config.py (annotated)

``` python
# few-shot ì˜ˆì‹œ ëª©ë¡
# - LLMì—ê²Œ "ë‹µë³€ì˜ í˜•ì‹/í†¤/ë””í…Œì¼ ìˆ˜ì¤€"ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ìƒ˜í”Œ Q/Aë“¤ì…ë‹ˆë‹¤.
# - FewShotChatMessagePromptTemplateì—ì„œ examplesë¡œ ì£¼ì…ë©ë‹ˆë‹¤.
# - ì‹¤ë¬´ì—ì„œëŠ”: ê°€ì¥ ëŒ€í‘œì ì¸ ì§ˆë¬¸ ìœ í˜•(Top N) + ì›í•˜ëŠ” í¬ë§·ì„ ê°•ì œí•˜ëŠ” ì˜ˆì‹œë¥¼ ë„£ìŠµë‹ˆë‹¤.
answer_examples = [
    {
        "input": "ì†Œë“ì€ ì–´ë–»ê²Œ êµ¬ë¶„ë˜ë‚˜ìš”?",
        "answer": """ì†Œë“ì„¸ë²• ì œ 4ì¡°(ì†Œë“ì˜ êµ¬ë¶„)ì— ë”°ë¥´ë©´ ì†Œë“ì€ ì•„ë˜ì™€ ê°™ì´ êµ¬ë¶„ë©ë‹ˆë‹¤.
1. ì¢…í•©ì†Œë“
    - ì´ ë²•ì— ë”°ë¼ ê³¼ì„¸ë˜ëŠ” ëª¨ë“  ì†Œë“ì—ì„œ ì œ2í˜¸ ë° ì œ3í˜¸ì— ë”°ë¥¸ ì†Œë“ì„ ì œì™¸í•œ ì†Œë“ìœ¼ë¡œì„œ ë‹¤ìŒ ê° ëª©ì˜ ì†Œë“ì„ í•©ì‚°í•œ ê²ƒ
    - ê°€. ì´ìì†Œë“
    - ë‚˜. ë°°ë‹¹ì†Œë“
    - ë‹¤. ì‚¬ì—…ì†Œë“
    - ë¼. ê·¼ë¡œì†Œë“
    - ë§ˆ. ì—°ê¸ˆì†Œë“
    - ë°”. ê¸°íƒ€ì†Œë“
2. í‡´ì§ì†Œë“
3. ì–‘ë„ì†Œë“
"""
    },
    {
        "input": "ì†Œë“ì„¸ì˜ ê³¼ì„¸ ê¸°ê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "answer": """ì†Œë“ì„¸ë²• ì œ5ì¡°(ê³¼ì„¸ê¸°ê°„)ì— ë”°ë¥´ë©´,
ì¼ë°˜ì ì¸ ì†Œë“ì„¸ì˜ ê³¼ì„¸ê¸°ê°„ì€ 1ì›” 1ì¼ë¶€í„° 12ì›” 31ì¼ê¹Œì§€ 1ë…„ì…ë‹ˆë‹¤.
í•˜ì§€ë§Œ ê±°ì£¼ìê°€ ì‚¬ë§í•œ ê²½ìš°ëŠ” 1ì›” 1ì¼ë¶€í„° ì‚¬ë§ì¼ê¹Œì§€,
ê±°ì£¼ìê°€ í•´ì™¸ë¡œ ì´ì£¼í•œ ê²½ìš° 1ì›” 1ì¼ë¶€í„° ì¶œêµ­í•œ ë‚ ê¹Œì§€ ì…ë‹ˆë‹¤."""
    },
    {
        "input": "ì›ì²œì§•ìˆ˜ ì˜ìˆ˜ì¦ì€ ì–¸ì œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
        "answer": """ì†Œë“ì„¸ë²• ì œ143ì¡°(ê·¼ë¡œì†Œë“ì— ëŒ€í•œ ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦ì˜ ë°œê¸‰)ì— ë”°ë¥´ë©´,
ê·¼ë¡œì†Œë“ì„ ì§€ê¸‰í•˜ëŠ” ì›ì²œì§•ìˆ˜ì˜ë¬´ìëŠ” í•´ë‹¹ ê³¼ì„¸ê¸°ê°„ì˜ ë‹¤ìŒ ì—°ë„ 2ì›” ë§ì¼ê¹Œì§€
ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦ì„ ê·¼ë¡œì†Œë“ìì—ê²Œ ë°œê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ë§Œ, í•´ë‹¹ ê³¼ì„¸ê¸°ê°„ ì¤‘ë„ì— í‡´ì§í•œ ì‚¬ëŒì—ê²ŒëŠ” í‡´ì§í•œ ë‚ ì˜ ë‹¤ìŒ ë‹¬ ë§ì¼ê¹Œì§€ ë°œê¸‰í•´ì•¼ í•˜ë©°,
ì¼ìš©ê·¼ë¡œìì— ëŒ€í•˜ì—¬ëŠ” ê·¼ë¡œì†Œë“ì˜ ì§€ê¸‰ì¼ì´ ì†í•˜ëŠ” ë‹¬ì˜ ë‹¤ìŒ ë‹¬ ë§ì¼ê¹Œì§€ ë°œê¸‰í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
ë§Œì•½ í‡´ì‚¬ìê°€ ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦ì„ ìš”ì²­í•œë‹¤ë©´ ì§€ì²´ì—†ì´ ë°”ë¡œ ë°œê¸‰í•´ì•¼ í•©ë‹ˆë‹¤."""
    },
]
```
